% !Mode:: "TeX:UTF-8"

\chapter{Deep Learning Based Model for Runtime Analysis}
In this chapter we describe our proposed deep learning based method for 3D IC runtime analysis.
First, we briefly review the work of \cite{Zhang2016Fast}, for their model
will be partially used in our work.
Then we describe the models used in our proposed method. They are modified from some modern
CNNs used in computer vision jobs such as object detection and face recognition.
We will specify these models' structure, and how they differ from their inceptions
to be fitted into 3D IC reliability runtime analysis jobs.
Finally, we apply this method to the dataset in \cite{Zhang2016Fast},
get a promising accuracy with economic computing cost.
The details of this experiment are demonstrated in the last section.

\section{A Fully-Connected ANN Model}
Since FEM based models are computational expensive, we may wish to figure out whether
it exist a much simpler and faster map $f: T \to F_{\left\{ x,y,z \right\}}$
\footnote{$T$ and $F_{\left\{ x,y,z \right\}}$ are notations in section \ref{sec::thermal}, which means
$f$ is a map from IC temperature distribution to its relevant thermal stress}
that could be used in runtime scenario.
\cite{Zhang2016Fast} proposed a two-layers fully connected ANN model with hand-crafted
feature extraction, then train this model by large amount of $(T \to F_{\left\{x,y,x\right\}})$ data
generated off-line by FEM based model. 
The ANN model achieved accuracy of RMSE(Rooted Mean Square Error) $0.0779$
in the normed test set, and fast enough 
($300 TSVs / 0.06s$ @ 2.4 GHz, 1 core) to be applied
in runtime scenario due to simplicity of the ANN model they used.
The highlights of their work include:

\subsection{FEM Generated Temperature-Stress Dataset} \label{sec::FEM-data}
To best of our current knowledge, \cite{Zhang2016Fast} is the very first work that provide a
accuracy and substantial dataset of 3D IC temperature-stress info.
They have built a two-layer 3D IC model with $12\times12$ TSVs uniformly placed
in the whole chip using the FEM based software \textit{COMSOL}.
The size of whole chip is $1cm\times1cm\times300\mu m$, and it is
divided into $4\times4$ same sized blocks to represent $16$ cores,
both of the two layers are $1cm\times1cm\times100\mu m$. 
For the TSV structure, they set the values of $r_i$ and $r_o$
% TODO: demostrate $r_i$ and $r_o$
as $20\mu m$ and $24\mu m$, respectively. They also couple
the solid heat conduction field and the solid mechanical field.

Applying different thermal distribution to above model, they generated $2736$
pairs of temperature-stress data that can be used in neural network training.
We denote this dataset as $\mathcal{D}$, 
and its quality as $N_\mathcal{D}=2736$.
The format of their data is like table \ref{tab::t-sformat}
\footnote{These data in tabular are randomly selected in dataset of \cite{Zhang2016Fast}}.
Note that the coordinate of $z$ is always $0$, because the dies in 3D IC are
extremely thin.

\begin{table}[htb]
\centering
\begin{tabularx}{30em}{*{3}{>{\centering\arraybackslash}X} *{2}{|c}}
    \toprule
    \multicolumn{3}{c|}{Coordinate($m$)} & 
    \multirow{2}{*}{Temperature($K$)} & 
    \multirow{2}{*}{Stress($N/m^2$)} \\
    \cmidrule{1-3}
    x & y & z && \\
    \midrule
    8.4631E-5 & 9.5289E-5 & 0 & 359.1351 & 31150.4105 \\
    2.0725E-4 & 1.2576E-4 & 0 & 359.0865 & 58634.6722 \\
    9.5824E-5 & 2.0547E-4 & 0 & 359.0865 & 62021.0153 \\
    \bottomrule
\end{tabularx}
\caption{Format of Temperature-Stress Dataset}
\label{tab::t-sformat}
\end{table} 

\subsection{Hand-Crafted Feature Extraction} \label{sec::hc-fe}
Based on these observations:
\begin{itemize}
    % \setlength\itemsep{1em}
    \item Reliability of 3D IC with TSV structure strongly depends on 
          the \textit{maximum} of thermal stress;
    \item The maximum stress always appears within a certain distance from the TSV center.
          Large mount of temperature informations are redundance;
    \item Having a round structure,
          the TSV and the areas around it have a \textit{rotational symmetry
          property}: two different thermal/stress maps can be the exactly
          the same after a certain amount of rotation;
\end{itemize}
% TODO: add image for TSV rotation
\cite{Zhang2016Fast} proposed a feature extraction process: rotate the temperature distribution
around each TSVs by aligning their maximum temperature line to eliminate the rotation symmetry,
then interpolate these data points to a continuous true value matrix, finally select temperature
values uniformly from this interpolated matrix by the Cartesian distance. The procedure can be
illustrated as figure TODO.

After above precess, the raw text data can be transfered to vectors 
$\mathbf{x}_{i=1,\cdots,N_\mathcal{D}} \in \mathbb{R}^{320}$, 
with relevant maximum stress 
$y_{i=1,\cdots,N_\mathcal{D}} \in \mathbb{R}$,
we may train the ANN model with these dataset.

\subsection{A Fast Fully-Connected ANN Model}
The ANN model used in this method uses a simple two layers structure. 
The input layer gets $55$ neurons, and hidden layer $15$ neurons, finally a $1$ neuron layer
to output the approximated maximum stress value.
Training with $2400$ samples randomly selected from whole temperature-stress dataset,
the model achieves the accuracy of RMSE(Rooted Mean Square Error) $0.0779$ in the $300$ samples test set,
which is also randomly selected with no recurrence to training set, after $1000$ iterations.

\section{Automatic Feature Extraction via CNN}
Although the hand-crafted feature extraction procedure in \ref{sec::hc-fe} is 
practically proved working in 3D IC reliability analysis jobs,
we may archive a better accuracy by refine this feature extraction procedure.
We believe that the room of refinement still exists, for these reasons:
\begin{itemize}
    % \setlength\itemsep{1em}
    \item Since one may not intuitively give the stress value $y\in\mathbb{R}$
    by a temperature distribution, the feature mapping relationship could be
    much more complex than na\"ive uniform selection and rotational symmetry proposed
    in section \ref{sec::hc-fe};
    
    \item Rotating input data is a common data augmentation procedure in neural
    network training\cite{Goodfellow2016Deep}, 
    so the maximum temperature line alignment could be unnecessary,
    even harm to the accuracy of result;
    
    \item As described in section \ref{sec::hc-fe}, maximum stress is not related to
    the whole temperature distribution but little around TSVs. But in fully-connected
    ANN model, the output $\mathbf{y}\in\mathbb{R}^n$ depends on all of model input 
    $\mathbf{x}\in\mathbb{R}^m$. This may introduce strong noise into result.
    
    \item Hand-crafted feature extraction makes the train-predict procedure not
    end-to-end. One have to design such extraction to make model work, instead of
    let the model learn the feature itself.
\end{itemize}

As described in section \ref{sec::intro_cnn}, convolutional network has been proved as
an effective local feature extraction method. Due to its property of sparse interaction
and parameter sharing, CNN model could be quite suitable for 3D IC reliability analysis jobs.

\subsection{Data Preprocessing}
In this work, we use the same temperature-stress dataset in \cite{Zhang2016Fast}. 
For facilitating the procedure of convolution feature extraction, 
the inputs of CNN should be well formatted. 
The data preprocessing steps are listed as following:

\begin{description}[labelsep=0.5em]
    \item[Data Interpolation] Follow \cite{Zhang2016Fast}, we interpolate the $N$ data points 
    around each TSV $\left(x,y,temp\right)_{1,\cdots,N}$
    to a temperature matrix $\mathbf{T}\in\mathbb{R}^2$.
    Note that distribution of valid data points generated in FEM model is dense in
    the area around TSVs, and it becomes sparser where far from TSVs.
    % TODO:  add image of temperature data points distribution
    Based on this observation, we only take $N$ temperature points whose distance to its nearing TSV 
    $r \le r_{thresh} \times l$ for interpolation. Where $l$ denotes the distance of adjacent TSVs,
    $r_{thresh}$ the distance threshold ratio for data points selection. \label{itm::interp}
    In this work, we set $r_{thresh}=0.2$, and get $N=91$ as a result.
    Consider $N$ is not large enough, the interpolation size is set to $28\times28$,
    same to the MINST handwritten dataset \cite{lecun1998gradient}.
       
    \item[Train-Test Splitting] Since \cite{Zhang2016Fast} didn't give their partition of dataset $\mathcal{D}$,
    we use the \texttt{scikit-learn} toolbox to split the whole dataset into 
    train set $\mathcal{D}_{\textrm{train}}$ and test set $\mathcal{D}_{\textrm{test}}$
    by randomly selection.
    The ratio of train/test is $0.7 / 0.3$, precisely there are 
    $N_{\mathcal{D}_{\textrm{train}}}=1915$ samples in train set
    and $N_{\mathcal{D}_{\textrm{test}}}=821$ samples in test set.
    
    \item[Whitening Input] It has been long known that the network training converges faster 
    if its inputs are whitened â€“ i.e., 
    linearly transformed to have zero means and unit variances, and decorrelated \cite{orr2003neural}.
    Thus we perform this linear transport to interpolated data matrix: 
    $\mathbf{T}^* = \frac{\mathbf{T} - \mathbf{M}}{\sigma}$, 
    where $\mathbf{M}$ is the mean value matrix of the training set \footnote{Note the $\mathbf{M}$
    here is the capital Greek letter $\mu$ instead of capital English letter m},
    $\sigma$ the variances of the training set.
    Also note that $\mu_{\left(x,y\right)} = \frac{1}{N_{\mathcal{D}_{\textrm{train}}}} \sum_{i=1}^{N_{\mathcal{D}_{\textrm{train}}}} 
    t^{\left(i\right)}_{\left(x,y\right)}$, where $i$ is the index of training samples.
    
    \item[Scaling Output] Since the absolute value of target output, the thermal stress, is
    quite large, it must be normed to avoid large gradient in back propagation.
    The norm procedure to $y$ is quite straightforward:
    $\mathbf{y}^* = \mathbf{y} / \max \left(\left|y_{1,\cdots,N_{\mathcal{D}_{\textrm{train}}}}\right|\right)$.
\end{description}
Note that when whitening and scaling whole dataset $\mathcal{D}$, we could only use
$\mathcal{D}_{\textrm{train}}$ to calculate $\mathbf{M}$, $\sigma$, $y_{max}$, and apply them
to both $\mathcal{D}_{\textrm{train}}$ and $\mathcal{D}_{\textrm{test}}$. Because in practical situations,
$\mathcal{D}_{\textrm{test}}$ is unknown to the model.

We denote these processed dataset as $\mathcal{D}_{\textrm{train}}^*$, $\mathcal{D}_{\textrm{test}}^*$, respectively.
The samples in them are formatted as $\left(\mathbf{T}^*, y^*\right)^{(i)}_{i=1,\cdots, N}$,
where temperature map $\mathbf{T}^*$ are $28 \times 28$ matrix, and normed max stress $y ^ *$ are scalars.
These form ideal ``x-y'' maps to train neural networks.

\subsection{A LeNet Based Model}
LeNet is a very early CNN structure for handwriting recognition \cite{lecun1998gradient}. Since the temperature map
$\mathbf{T}^*$ in $\mathcal{D}_{\textrm{train}}^*$ and $\mathcal{D}_{\textrm{test}}^*$ get the same size to MINST dataset,
LeNet becomes the very first choice to perform feature extraction. 

The very origin purpose of LeNet is recognizing a $28 \times 28$ pixes gray scale picture of hand writing digits (0 to 9),
decide which digit that picture represents for. The input of LeNet is a $28 \times 28$ matrix $\mathbf{X} \in \mathbb{R}^2$,
output a $10$ length vector $\mathbf{p}$. $p_{i=0,1,\cdots,9} \in [0, 1]$ is the \textit{probability} of $\mathbf{X}$ to be
digit $i$. Usually this output follows a \textit{maxout} layer to output that $i$ with maximum probability.
The structure of LeNet is demonstrated in table \ref{tab::LeNet}.
\begin{table}[htb]
    \centering
    \begin{tabular}{cll}
        \toprule
        Layer ID & Layer Type & Configuration \\
        \midrule
        conv1 & Convolution & $\left(5\times5\right)$, $20$ \\
        sig1  & Activation  & Sigmoid \\
        pool1 & Pooling     & Max, $\left(2\times2\right)$, $2$ \\
        conv2 & Convolution & $\left(5\times5\right)$, $50$ \\
        sig2  & Activation  & Sigmoid \\
        pool2 & Pooling     & Max, $\left(2\times2\right)$, $2$ \\
        flat  & Flatten     & \\
        fc1   & Fully-Connected & $500$ \\
        sig3  & Activation  & Sigmoid \\
        fc2   & Fully-Connected & $10$ \\
        maxout & Maxout     & \\
        \bottomrule
    \end{tabular}
    \caption{Structure of Original LeNet (top to down)}
    \label{tab::LeNet}
\end{table}
Note that in table \ref{tab::LeNet}, the meanings of ``Configuration'' of each layer types are:
\begin{description}[labelsep=0.5em]
    \item[Convolution] Convolution kernel size, number of channels
    \item[Activation] Type of activation function
    \item[Pooling] Pooling type, pooling kernel size, stride of pooling kernel
    \item[Fully-Connected] number of hidden units
\end{description}
All Convolution kernels stride with step of $(1 \times 1)$ (vertical, horizontal),
and with no padding.
And the ``Flatten'' layer transforms the output of pool2, which is the feature been extracted
with shape $batch\_size \times channel\_num \times height \times width \in \mathbb{R}^4$, into
$batch\_size \times channel\_num \cdot height \cdot width \in \mathbb{R}^2$ as input of following
fully-connected layers.

To fit into the 3D IC runtime reliability analysis job with samples as 
$\left(\mathbf{T}^*, y^*\right)^{(i)}_{i=1,\cdots, N}$,
we modified the original LeNet as following:
\begin{itemize}
    \item The number of channels in layer fc1 is changed to $50$, to maintain a same order of magnitude
    with the fully-connected ANN model in \cite{Zhang2016Fast}. 
    And number of channels in layer fc2 is changed to $1$, because our model
    only needs to output a scalar as the approximation of normed maximum stress $y ^ *$.
    \item The activation layers are all changed into tanh activated, formally
    \begin{equation}
    \tanh (x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
    \end{equation}
    because we experimentally found this will significantly improve the accuracy of result.
    \item Since we don't need to pick an index with maximum probability anymore, the maxout
    layer could be removed.
\end{itemize}
After all, our LeNet based model for 3D IC runtime reliability analysis job is structured as table \ref{tab::LeNet'}.
\begin{table}[htb]
    \centering
    \begin{tabular}{cll}
        \toprule
        Layer ID & Layer Type & Configuration \\
        \midrule
        conv1 & Convolution & $\left(5\times5\right)$, $20$ \\
        tanh1  & Activation  & Tanh \\
        pool1 & Pooling     & Max, $\left(2\times2\right)$, $2$ \\
        conv2 & Convolution & $\left(5\times5\right)$, $50$ \\
        tanh2  & Activation  & Tanh \\
        pool2 & Pooling     & Max, $\left(2\times2\right)$, $2$ \\
        flat  & Flatten     & \\
        fc1   & Fully-Connected & $50$ \\
        tanh3  & Activation  & Tanh \\
        fc2   & Fully-Connected & $1$ \\
        \bottomrule
    \end{tabular}
    \caption{Structure of Modified LeNet (top to down)}
    \label{tab::LeNet'}
\end{table}
In our experiment \ref{sec::exper}, this model outperforms the fully-connected ANN model in \cite{Zhang2016Fast}
with great advantage.

\subsection{A Tiny VGG-Net Based Model}
VGG-Net is a very deep CNN architecture proposed by Visual Geometry Group (VGG) of Oxford \cite{simonyan2014very}.
Its original purpose is to solve the ImageNet Large-ScaleVisual Recognition Challenge
(ILSVRC-2014) image classification task \cite{russakovsky2015imagenet}.
Equipped with very small convolution kernels ($3 \times 3$), the VGG-Net could exploit from very deep
stacked convolution layers.

VGG-Net has several variants, with depth from 11 weight layers (8 conv. and 3 FC layers) 
to 19 weight layers in the network (16 conv. and 3 FC layers). For the input size of $\mathcal{D}^*$
is much smaller to VGG-Net's input size ($28 \times 28$ vs. $224 \times 224$),
we select the smallest variant as our inception to make sure data samples in $\mathcal{D}^*$ could ``feed'' the net.
Structure of this variant is demonstrated in table \ref{tab::VGG-Net}.

\begin{table}[htb]
    \centering
    \begin{tabular}{cll}
        \toprule
        Layer ID & Layer Type & Configuration \\
        \midrule
        conv1 & Convolution & $\left(3\times3\right)$, $64$ \\
        pool1 & Pooling       & Max, $\left(2\times2\right)$, $2$ \\
        conv2 & Convolution & $\left(3\times3\right)$, $128$ \\
        pool2 & Pooling       & Max, $\left(2\times2\right)$, $2$ \\
        conv3 & Convolution & $\left(3\times3\right)$, $256$ \\
        conv4 & Convolution & $\left(3\times3\right)$, $256$ \\
        pool3 & Pooling       & Max, $\left(2\times2\right)$, $2$ \\
        conv5 & Convolution & $\left(3\times3\right)$, $512$ \\
        conv6 & Convolution & $\left(3\times3\right)$, $512$ \\
        pool4 & Pooling       & Max, $\left(2\times2\right)$, $2$ \\
        conv7 & Convolution & $\left(3\times3\right)$, $512$ \\
        conv8 & Convolution & $\left(3\times3\right)$, $512$ \\
        pool5 & Pooling       & Max, $\left(2\times2\right)$, $2$ \\
        fc1   & Fully-Connected & $4096$ \\
        fc2   & Fully-Connected & $4096$ \\
        fc3   & Fully-Connected & $1000$ \\
        sf    & Soft-Max      & \\
        \bottomrule
    \end{tabular}
    \caption{Structure of shallow VGG-Net}
    \label{tab::VGG-Net}
\end{table}

Note that we omit the activation layers in \ref{tab::VGG-Net} for conciseness. 
Precisely each ``Convolution'' layer is a convolution unit structured as \ref{tab::VGG-CU}.
\begin{table}[htb]
    \centering
    \begin{tabular}{cll}
        \toprule
        Layer ID & Layer Type & Configuration \\
        \midrule
        conv\textit{i} & Convolution & \textit{conv config} \\
        relu\textit{i} & Activation  & ReLU \\
        \bottomrule
    \end{tabular}
    \caption{Convolution unit in \ref{tab::VGG-Net}}
    \label{tab::VGG-CU}
\end{table}
The activation layers in these convolution units are equipped with Rectified Linear Units (ReLU)
activation function, formally $g(x) = \max (0, x)$. 
Rectified linear units are easy to optimize because they are so similar to linear units. 
The only difference between a linear unit and a rectified linear unit is that a rectified linear unit outputs zero across half its domain. 
This makes the derivatives through a rectified linear unit remain large whenever the unit is active.
The gradients are not only large but also consistent. The second derivative of the rectifying operation is $0$ almost everywhere, 
and the derivative of the rectifying operation is $1$ everywhere that the unit is active. 
This means that the gradient direction is far more useful for learning than it would be with activation functions
that introduce second-order effects.
    
The images in ILSVRC-2014 are classified into $1000$ classes (human, dogs, cars, etc.),
so the fc3 layer in VGG-Net gets $1000$ hidden neurons to output the probability of input being
such class.
To correctly output the desired maximum stress approximation,
we replace all fully-connected layers in this structure to flat and bellow layers in configuration
\ref{tab::LeNet'}.

\section{CNN-Stress: an End-to-End Process}

\section{Experiments} \label{sec::exper}
